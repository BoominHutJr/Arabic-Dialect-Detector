{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Libraries and Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "yh70rnlFmZ_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r /content/drive/MyDrive/Arabic-Dialect-Detector/requirements.txt"
      ],
      "metadata": {
        "id": "t028WpCgmzkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7fb9MriCLx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorWithPadding\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "Aqr_h4vTpjAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Design"
      ],
      "metadata": {
        "id": "RaueJZQNmalK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Quantization configs"
      ],
      "metadata": {
        "id": "69tEVdSVtsIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bnb library quantizes model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# peft config for quantized model\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # SEQ_CLS for dialect detection, SEQ_2_SEQ_LM for translation\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "3gb7gkWpmaWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Setup"
      ],
      "metadata": {
        "id": "pV-KKT5Ftpqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           quantization_config=bnb_config,\n",
        "                                                           num_labels=5,\n",
        "                                                           device_map =\"auto\")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "dw6IRyuGtzdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "XNFo0ND1ma6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "VUwVbKMNk5sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "training_data_path = r\"/content/drive/MyDrive/Arabic-Dialect-Detector/src/data/Arabic_dialect.csv\"\n",
        "dataset = load_dataset(\"csv\", data_files=training_data_path)\n",
        "\n",
        "dataset = dataset.rename_column(\"result\", \"label\")\n",
        "dataset = dataset.rename_column(\"t\", \"text\")\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=[\"text\", \"label\"])"
      ],
      "metadata": {
        "id": "fuCD5cmGBcYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.4, seed= 42)\n",
        "test_val = train_test[\"test\"].train_test_split(test_size=0.5, seed= 42)\n",
        "\n",
        "columns_to_remove = ['Date','User', 'Tweet']\n",
        "# format data into DatsetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_test[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": test_val[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"test\": test_val[\"test\"].remove_columns(columns_to_remove)\n",
        "})\n",
        "dataset"
      ],
      "metadata": {
        "id": "O_BlBi3pmbJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the pad token if it's missing\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "# Tokenization function\n",
        "max_l = 128\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128)\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function,\n",
        "                                 batched=True,\n",
        "                                 load_from_cache_file=False,\n",
        "                                 batch_size = 16)\n",
        "\n",
        "print(tokenized_datasets)\n",
        "# tokenized_datasets['train'][0]"
      ],
      "metadata": {
        "id": "dZ3wJ8huw09P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = ['label', 'text']\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(\n",
        "    lambda examples: {\"labels\": examples[\"label\"]},\n",
        ")\n",
        "\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": tokenized_datasets[\"validation\"].remove_columns(columns_to_remove),\n",
        "    \"test\": tokenized_datasets[\"test\"].remove_columns(columns_to_remove),\n",
        "})\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "tww2UxAGWh69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Padding Token: {tokenizer.pad_token}\")\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(f\"Padding Token ID: {tokenizer.pad_token_id}\")"
      ],
      "metadata": {
        "id": "yWVpMwGDhK4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "J5HkhK1gmbVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n"
      ],
      "metadata": {
        "id": "5QX8KfZubjXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tokenized_datasets[\"train\"][0])  # Check tokenized example\n"
      ],
      "metadata": {
        "id": "j0kVmZakbtYf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer, adding the compute_metrics argument\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# help memory please????\n",
        "torch.cuda.empty_cache()\n",
        "model.gradient_checkpointing_enable()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "uQu16QOymbqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "xf1EZ9QjoAFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "RNPjHpMToBFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}