{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh70rnlFmZ_a"
      },
      "source": [
        "\n",
        "\n",
        "# Libraries and Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t028WpCgmzkZ"
      },
      "outputs": [],
      "source": [
        "#%pip install -q -r /content/drive/MyDrive/Arabic-Dialect-Detector/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7fb9MriCLx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorWithPadding\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqr_h4vTpjAb"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaueJZQNmalK"
      },
      "source": [
        "# Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69tEVdSVtsIE"
      },
      "source": [
        " ### Quantization configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gb7gkWpmaWI"
      },
      "outputs": [],
      "source": [
        "# bnb library quantizes model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "# peft config for quantized model\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # SEQ_CLS for dialect detection, SEQ_2_SEQ_LM for translation\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-KKT5Ftpqa"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw6IRyuGtzdM"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           quantization_config=bnb_config,\n",
        "                                                           num_labels=5,\n",
        "                                                           device_map =\"auto\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNFo0ND1ma6x"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuCD5cmGBcYO"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "training_data_path = r\"/content/drive/MyDrive/Arabic-Dialect-Detector/src/data/Arabic_dialect.csv\"\n",
        "dataset = load_dataset(\"csv\", data_files=training_data_path)\n",
        "\n",
        "dataset = dataset.rename_column(\"result\", \"label\")\n",
        "dataset = dataset.rename_column(\"t\", \"text\")\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=[\"text\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_BlBi3pmbJo"
      },
      "outputs": [],
      "source": [
        "# split data\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.4, seed= 42)\n",
        "test_val = train_test[\"test\"].train_test_split(test_size=0.5, seed= 42)\n",
        "\n",
        "columns_to_remove = ['Date','User', 'Tweet']\n",
        "# format data into DatsetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_test[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": test_val[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"test\": test_val[\"test\"].remove_columns(columns_to_remove)\n",
        "})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lengths = [len(tokenizer.encode(text)) for text in dataset['train']['text']]\n",
        "print(f\"Average length: {np.mean(lengths):.2f}\")\n",
        "print(f\"95th percentile length: {np.percentile(lengths, 95):.2f}\")\n",
        "print(f\"Max length: {max(lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ3wJ8huw09P"
      },
      "outputs": [],
      "source": [
        "# Set the pad token if it's missing\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "# Tokenization function\n",
        "max_l = 128 #TODO fix based on stats\n",
        "def preprocess_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_l,\n",
        "        return_tensors=None,  \n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    tokenized[\"labels\"] = examples[\"label\"] \n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function,\n",
        "                                 batched=True,\n",
        "                                 load_from_cache_file=False,\n",
        "                                 batch_size = 16,\n",
        "                                 remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "print(tokenized_datasets)\n",
        "# tokenized_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tww2UxAGWh69"
      },
      "outputs": [],
      "source": [
        "columns_to_remove = ['label', 'text']\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(\n",
        "    lambda examples: {\"labels\": examples[\"label\"]},\n",
        ")\n",
        "\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": tokenized_datasets[\"validation\"].remove_columns(columns_to_remove),\n",
        "    \"test\": tokenized_datasets[\"test\"].remove_columns(columns_to_remove),\n",
        "})\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWVpMwGDhK4j"
      },
      "outputs": [],
      "source": [
        "print(f\"Padding Token: {tokenizer.pad_token}\")\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(f\"Padding Token ID: {tokenizer.pad_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tokenized_datasets[\"train\"][0]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5HkhK1gmbVF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX8KfZubjXb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQu16QOymbqx"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "# help memory please????\n",
        "torch.cuda.empty_cache()\n",
        "model.gradient_checkpointing_enable()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer, adding the compute_metrics argument\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1EZ9QjoAFc"
      },
      "source": [
        "Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNPjHpMToBFy"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
