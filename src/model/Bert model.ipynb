{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh70rnlFmZ_a"
      },
      "source": [
        "\n",
        "\n",
        "# Libraries and Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t028WpCgmzkZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -r C:\\Users\\Boomin\\Documents\\GitHub\\Arabic-Dialect-Detector\\requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ys7fb9MriCLx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Boomin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "C:\\Users\\Boomin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorWithPadding\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "#from google.colab import userdata\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Aqr_h4vTpjAb"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaueJZQNmalK"
      },
      "source": [
        "# Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69tEVdSVtsIE"
      },
      "source": [
        " ### Quantization configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gb7gkWpmaWI"
      },
      "outputs": [],
      "source": [
        "# bnb library quantizes model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# peft config for quantized model\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, # SEQ_CLS for dialect detection, SEQ_2_SEQ_LM for translation\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-KKT5Ftpqa"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw6IRyuGtzdM"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-mix\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           quantization_config=bnb_config,\n",
        "                                                           num_labels=5,\n",
        "                                                           device_map =\"auto\")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNFo0ND1ma6x"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VUwVbKMNk5sn"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# define tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_name\u001b[49m,                                        do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                                         use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m special_tokens_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msep_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     10\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens(special_tokens_dict)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "# define tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                        do_lower_case=False,\n",
        "                                        use_fast=True)\n",
        "special_tokens_dict = {\n",
        "    'cls_token': '[CLS]',\n",
        "    'sep_token': '[SEP]',\n",
        "    'pad_token': '[PAD]',\n",
        "    'mask_token': '[MASK]'\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuCD5cmGBcYO"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "training_data_path = r\"C:\\Users\\Boomin\\Documents\\GitHub\\Arabic-Dialect-Detector\\src\\data\\Arabic_dialect.csv\"\n",
        "dataset = load_dataset(\"csv\", data_files=training_data_path)\n",
        "\n",
        "dataset = dataset.rename_column(\"result\", \"label\")\n",
        "dataset = dataset.rename_column(\"t\", \"text\")\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=[\"text\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_BlBi3pmbJo"
      },
      "outputs": [],
      "source": [
        "# split data\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.4, seed= 42)\n",
        "test_val = train_test[\"test\"].train_test_split(test_size=0.5, seed= 42)\n",
        "\n",
        "columns_to_remove = ['Date','User', 'Tweet']\n",
        "# format data into DatsetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_test[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": test_val[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"test\": test_val[\"test\"].remove_columns(columns_to_remove)\n",
        "})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ3wJ8huw09P"
      },
      "outputs": [],
      "source": [
        "# Set the pad token if it's missing\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "# Tokenization function\n",
        "max_l = 128\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128)\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(preprocess_function,\n",
        "                                 batched=True,\n",
        "                                 load_from_cache_file=False,\n",
        "                                 batch_size = 16)\n",
        "\n",
        "print(tokenized_datasets)\n",
        "# tokenized_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tww2UxAGWh69"
      },
      "outputs": [],
      "source": [
        "columns_to_remove = ['label', 'text']\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(\n",
        "    lambda examples: {\"labels\": examples[\"label\"]},\n",
        ")\n",
        "\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"].remove_columns(columns_to_remove),\n",
        "    \"validation\": tokenized_datasets[\"validation\"].remove_columns(columns_to_remove),\n",
        "    \"test\": tokenized_datasets[\"test\"].remove_columns(columns_to_remove),\n",
        "})\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWVpMwGDhK4j"
      },
      "outputs": [],
      "source": [
        "print(f\"Padding Token: {tokenizer.pad_token}\")\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(f\"Padding Token ID: {tokenizer.pad_token_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5HkhK1gmbVF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX8KfZubjXb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j0kVmZakbtYf"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(tokenized_datasets[\"train\"][0])  # Check tokenized example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQu16QOymbqx"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer, adding the compute_metrics argument\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# help memory please????\n",
        "torch.cuda.empty_cache()\n",
        "model.gradient_checkpointing_enable()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1EZ9QjoAFc"
      },
      "source": [
        "Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNPjHpMToBFy"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(results)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
